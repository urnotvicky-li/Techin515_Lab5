{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core import Workspace, Dataset, Run\n",
    "\n",
    "# Define constants\n",
    "GESTURE_CLASSES = [\"O\", \"V\"]\n",
    "SEQUENCE_LENGTH = 100  # Adjust based on your data\n",
    "MODEL_SAVE_PATH = \"wand_model.h5\"\n",
    "\n",
    "def load_data_from_directory(directory):\n",
    "    \"\"\"Load all data files from a directory and return features and labels.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(GESTURE_CLASSES):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        \n",
    "        # Skip if not a directory\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "            \n",
    "        # Process all CSV files in the class directory\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                \n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Extract IMU data (x, y, z columns)\n",
    "                sequence = df[['x', 'y', 'z']].values\n",
    "                \n",
    "                # Process the sequence\n",
    "                if len(sequence) >= SEQUENCE_LENGTH:\n",
    "                    # Take the first SEQUENCE_LENGTH samples\n",
    "                    features.append(sequence[:SEQUENCE_LENGTH])\n",
    "                    labels.append(idx)\n",
    "                elif len(sequence) > 10:  # Minimum sequence length\n",
    "                    # Pad shorter sequences\n",
    "                    padded = np.pad(sequence, ((0, SEQUENCE_LENGTH - len(sequence)), (0, 0)), \n",
    "                                  mode='constant', constant_values=0)\n",
    "                    features.append(padded)\n",
    "                    labels.append(idx)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    \"\"\"Preprocess the data: reshape, normalize, and split into train/test sets.\"\"\"\n",
    "    # Reshape features to (samples, sequence_length * 3)\n",
    "    n_samples = features.shape[0]\n",
    "    features_flat = features.reshape(n_samples, -1)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features_normalized = scaler.fit_transform(features_flat)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_normalized, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build a neural network model for gesture classification.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Get the run context\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    if hasattr(run, \"experiment\"):\n",
    "        # We're in an AzureML job\n",
    "        ws = run.experiment.workspace\n",
    "    else:\n",
    "        # We're running locally (e.g., in JupyterLab)\n",
    "        ws = Workspace.from_config()  # assumes config.json is present or you're in a bound environment\n",
    "    \n",
    "    dataset = Dataset.File.from_files(path=(ws.get_default_datastore(), 'UI/2025-04-07_211604_UTC/**'))\n",
    "    \n",
    "    # Mount the dataset\n",
    "    mount_context = dataset.mount()\n",
    "    mount_context.start()\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        features, labels = load_data_from_directory(mount_context.mount_point)\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"No data found. Please check your data directory.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Loaded {len(features)} samples across {len(GESTURE_CLASSES)} classes\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(features, labels)\n",
    "        \n",
    "        # Build and train model\n",
    "        input_shape = X_train.shape[1]\n",
    "        model = build_model(input_shape, len(GESTURE_CLASSES))\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(\"Evaluating model...\")\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        # Log metrics to Azure ML\n",
    "        run.log(\"Test Accuracy\", test_acc)\n",
    "        run.log(\"Test Loss\", test_loss)\n",
    "        \n",
    "        # Save model\n",
    "        model.save(MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "        \n",
    "    finally:\n",
    "        # Always unmount the dataset\n",
    "        mount_context.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
